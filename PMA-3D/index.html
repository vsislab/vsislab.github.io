
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Parameter-level Mask-based Adapter for 3D Continual Learning with Vision-Language Model</title>
    <!-- CSS -->
        <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen" />
    <!-- ENDS CSS -->

        <!--script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');

        </script-->

        <!-- ENDS JS -->  
  </head> 
  <body>
    <!-- MAIN -->
    <div id="main">

            <div id="match-nav-wrapper">
                <div id="match-nav-bar">

                    <table>
                        <thead>
                            <tr>
                                <!--th width="10%"><a href="#">Home</a></th-->
                                <!--th width="30%"><a href="#video-explicit">Demonstration of the explicit method</a></th>
                                <th width="30%"><a href="#video-implicit">Demonstration of the implicit method</a></th-->
                                <!--th width="15%"></th-->
                            </tr>
                        </thead>
                    </table>

                </div>
            </div>

      <!-- HEADER -->
      <div id="home-header">
      
            <div id="match-wrapper">

                <!--h5><a href="#"><img src="image/logo.png"></a></h5-->

              

                <h2>Parameter-level Mask-based Adapter for 3D Continual Learning with Vision-Language Model</h2>
                <center>
                  Longyue Qian, Mingxin Zhang, Qian Zhang, Lin Zhang, Teng Li,  Wei Zhang<sup>*</sup><br />
                  <font size=2>School of Control Science and Engineering, Shandong University</font>    
                </center>
                <br><strong>Abstract</strong>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                As 3D object classification becomes increasingly essential in various applications, the need for models to continuously adapt to new scenarios is more crucial than ever. However, most existing continual learning methods primarily concentrate on 2D tasks, making their direct application to 3D tasks challenging due to the inherent domain gap. To address this issue, we propose Parameter-level Mask-based Adapter (PMA) for 3D continual learning with Vision-Language Model. Our approach leverages the power of Contrastive Language-Image Pre-Training (CLIP) to extract rich multimodal features by projecting 3D point clouds into multi-angle depth maps, which are then processed by the 2D encoder aligned with rendered images. The core of our method is a parameter-level adapter with learnable masks that effectively integrates 3D point cloud features with 2D features. This mechanism flexibly selects the most relevant subsets of weights for each task, allowing subsequent tasks to utilize the learned weights without the need for updates, which effectively balances stability and plasticity during continual learning. Experimental results show that our method significantly outperforms existing state-of-the-art techniques.
		
                <div id="hiddenContent">
                   <p style="text-align: center;">Code Download: <a href="https://pan.baidu.com/s/1uMZh8OeS8G1_HWCGUaAX4w?pwd=p757">here</a></p>
                </div>
                <script>
                    // 获取下载链接元素
                    var downloadLink = document.getElementById('downloadLink');
                    // 为下载链接添加点击事件监听器
                    downloadLink.addEventListener('click', function () {
                   // 当点击下载链接时，将隐藏元素显示出来
		  setTimeout(function () {
                   var hiddenContent = document.getElementById('hiddenContent');
                   hiddenContent.style.display = 'block';
	           }, 2000);
                    });
                  </script>

                <h1>Overview</h1>
                <div id="overview"><img src="image/overview3.png" style="width: 100%; height: auto;"></div>
		    <div style="text-align:justify; text-justify:inter-ideograph;">
			    We project 3D point clouds into multi-angle depth maps, which are processed by a 2D encoder alongside rendered images. As the core of our method, the Parameter-level Mask-based Adapter (PMA) achieves parameter-level isolation, allowing flexible selection of the most relevant weights for each task without destroying previously learned knowledge for continual learning. This adapter extracts features from the two modalities through its 3D and 2D parts, which are then aggregated to form global features used in contrastive learning with text features for 3D classification tasks.
		        
                    </div>
	            
			    
	            <br><strong>Parameter-level Mask-based Adapter</strong>

		    <div style="text-align:center;">
			<div id="incre"><img src="image/inc.png" style="width: 70%; height: auto;"></div>
			    <div style="text-align:justify; text-justify:inter-ideograph;">
				In each task, all parameters are evaluated for their importance to the current task, indicated by the bouncing squares. Parameters with scores above the top-c$\%$  threshold, as shown within the dashed lines, are selected to form an optimal parameter set for the task at hand. These selected parameters are then frozen in subsequent tasks to preserve learned knowledge and mitigate forgetting.
		        
		    </div>



		<h1>Performance Comparisons of the Proposed Method and Baselines</h1>	
		    <div id="compare2baseline"><img src="image/table1.png" style="width: 100%; height: auto;"></div>
		    For each method, the top row represents Accuracy(Acc), indicating individual task performance, while the bottom row shows Macro Accuracy(MAcc), reflecting overall performance across all tasks. The best results are highlighted in bold.
		<h1>Hyper-parameter Analysis:</h1>	
		    <div id="para"><img src="image/para.png" style="width: 100%; height: auto;"></div>
		    

		<h1>Ablation Study</h1>	
		    <div id="ablation"><img src="image/ablation.png" style="width: 60%; height: auto;"></div>

		    

              </div>
      <!-- ENDS Footer -->
  </body>
</html>
