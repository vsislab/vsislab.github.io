
<html>
<title>Autonomous Robot Navigation Based on Multi-Camera Perception</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--Import Google Icon Font-->
      <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <!--Import materialize.css-->
      <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
      <!--Let browser know website is optimized for mobile-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>    

<body>
	<br />
<center><h4 class="black-text" style="padding-left: 210px; padding-right: 210px;">Autonomous Robot Navigation Based on Multi-Camera Perception</h4></center>
<center><h6 class="gray-text">Kunyan Zhu, Wei Chen, Wei Zhang, Ran Song, and Yibin Li</h6></center>
    <center><h8>School of Control Science and Engineering, Shandong University&nbsp;&nbsp;</h8></center>
<p style="padding-left: 210px;"><b>Abstract</b></p>       
<p style="padding-left: 210px; padding-right: 210px; text-align:justify; text-justify:inter-ideograph;">
 In this paper, we propose an autonomous method
for robot navigation based on a multi-camera setup that takes
advantage of a wide field of view. A new multi-task network
is designed for handling the visual information supplied by the
left, central and right cameras to find the passable area, detect
the intersection and infer the steering. Based on the outputs
of the network, three navigation indicators are generated and
then combined with the high-level control commands extracted
by the proposed MapNet, which are finally fed into the
driving controller. The indicators are also used through the
controller for adjusting the driving velocity, which assists the
robot to adjust the speed for smoothly bypassing obstacles.
Experiments in real-world environments demonstrate that our
method performs well in both local obstacle avoidance and
global goal-directed navigation tasks.

</p>
<!--p style="padding-left: 210px;"><b>● Data: <a href="data.html" target="_blank">View our datasets</a></b></p-->
<center><img src="image/overview.jpg" width="35%">
<br> Overview of the proposed method.
</center>
<p style="padding-left: 210px;"><b>● Data & Code: </b>Coming soon!</p>
<p style="padding-left: 210px;"><b>● Video: </b>Coming soon!</p>
<!--h5 style="padding-left: 210px;"><blockquote>Explicit method for robot imitation learning</blockquote></h5>
<div style="padding-left: 210px; padding-right: 210px;"><hr></div-->
<center>

 <!--iframe width="640" height="360" src="https://www.youtube.com/embed/QvHZ1bpvwno" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe-->
    <br><br>
  
</center>

<!--iframe width="640" height="359" src="https://www.youtube.com/embed/uaJXXVa31wU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe-->
<br><br>


<!--h5 style="padding-left: 210px;"><blockquote>Implicit method for robot imitation learning</blockquote></h5
Simulation and real-world experiments video of our implicit method for robot imitation learning
Real world experiments video of our explicit method for robot imitation learning
-->
<!--Import jQuery before materialize.js-->
      <script type="text/javascript" src="js/jquery-2.1.1.min.js"></script>
      <script type="text/javascript" src="js/materialize.min.js"></script>
</body>
</html>