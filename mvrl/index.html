
<html>
<title>Autonomous Multi-View Navigation via Deep Reinforcement Learning </title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--Import Google Icon Font-->
      <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <!--Import materialize.css-->
      <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
      <!--Let browser know website is optimized for mobile-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>    

<body>
	<br />
<center><h4 class="black-text" style="padding-left: 300px; padding-right: 300px;">Autonomous Multi-View Navigation via Deep Reinforcement Learning</h4></center>
<center><h6 class="gray-text">Xueqin Huang, Wei Chen, Wei Zhang, Ran Song, Jiyu Cheng, and Yibin Li</h6></center>
    <center><h8>School of Control Science and Engineering, Shandong University&nbsp;&nbsp;</h8></center>
<p style="padding-left: 210px;"><b>Abstract</b></p>       
<p style="padding-left: 210px; padding-right: 210px; text-align:justify; text-justify:inter-ideograph;"><h7>
In this paper, we propose a novel deep reinforcement learning (DRL) system for the autonomous navigation of mobile robot that consists of three modules: map navigation, multi-view perception and multi-branch control. Our DRL system takes as the input a routed map provided by a global planner and three RGB images captured by a multi-camera setup to gather global and local information, respectively. In particular, we present a multi-view perception module based on an attention mechanism to filter out redundant information caused by the multi-camera sensing. We also replace raw RGB images with low-dimensional representations via a specifically designed network, which benefits a more robust sim2real transfer learning. Extensive experiments in both simulated and real-world scenarios demonstrate that our system outperforms the state-of-the-art approaches.

</p>
<!--p style="padding-left: 210px;"><b>● Data: <a href="data.html" target="_blank">View our datasets</a></b></p-->
<center><img src="image/introduction-v15.png" width="40%">
<br> Overview of the proposed multi-view navigation system via DRL.
</center>
<p style="padding-left: 210px;"><b>● Data & Code: </b>Coming soon!</p>
<p style="padding-left: 210px;"><b>● Video:</b></p>
<!--h5 style="padding-left: 210px;"><blockquote>Explicit method for robot imitation learning</blockquote></h5>
<div style="padding-left: 210px; padding-right: 210px;"><hr></div-->

<center>
<iframe width="840" height="480" src="https://www.youtube.com/embed/Rao_tJXMquo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br><br>
  



<!--h5 style="padding-left: 210px;"><blockquote>Implicit method for robot imitation learning</blockquote></h5
Simulation and real-world experiments video of our implicit method for robot imitation learning
Real world experiments video of our explicit method for robot imitation learning
-->
<!--Import jQuery before materialize.js-->
      <script type="text/javascript" src="js/jquery-2.1.1.min.js"></script>
      <script type="text/javascript" src="js/materialize.min.js"></script>
</body>
</html>
