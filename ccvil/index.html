<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Cross-Context Visual Imitation Learning from Demonstrations</title>
    <!-- CSS -->
        <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen" />
    <!-- ENDS CSS -->

        <!--script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
        </script-->

        <!-- ENDS JS -->  
  </head> 
  <body>
    <!-- MAIN -->
    <div id="main">

            <div id="match-nav-wrapper">
                <div id="match-nav-bar">

                    <table>
                        <thead>
                            <tr>
                                <th width="11%"><a href="#">Home</a></th>
                                <th width="13%"><a href="#demo-video">Video</a></th>
                                <!--th width="16%"><a href="#data-code">Data & Code</a></th-->
                                <th width="60%"></th>
                            </tr>
                        </thead>
                    </table>

                </div>
            </div>

      <!-- HEADER -->
      <div id="home-header">
      
            <div id="match-wrapper">

                <h5><a href="#"><img src="image/logo.png"></a></h5>

                <br><br>

                <h2>Cross-Context Visual Imitation Learning from Demonstrations</h2>
                <center>
                  Shuo Yang<sup>1</sup>, Wei Zhang<sup>1*</sup>, Weizhi Lu<sup>1</sup>, Hesheng Wang<sup>2</sup>, and Yibin Li<sup>1</sup><br>
<font size=2><sup>1</sup>School of Control Science and Engineering, Shandong University&nbsp;&nbsp;<sup>2</sup>Department of Automation, Shanghai Jiao Tong University</font>                
                </center>
				<br>International Conference on Robotics and Automation (ICRA), 2020<br>
                <br><strong>Abstract</strong>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                Imitation learning enables robots to learn a task by simply watching the demonstration of the task. Current imitation learning methods usually require the learner and demonstrator to occur in the same context. This limits their scalability to practical applications. In this paper, we propose a more general imitation learning method which allows the learner and the demonstrator to come from different contexts, such as different viewpoints, backgrounds, and object positions and appearances. Specifically, we design a robotic system consisting of three models: context translation model, depth prediction model and multi-modal inverse dynamics model. First, the context translation model translates the demonstration to the context of learner from a different context. Then combining the color observation and depth observation as inputs, the inverse model maps the multi-modal observations into actions to reproduce the demonstration, where the depth observation is provided by
a depth prediction model. By performing the block stacking tasks both in simulation and real world, we prove the cross-context learning advantage of  the proposed robotic system over other systems.
</p>
                <br>
                <h1>Overview</h1>
                <center><img src="image/overview.png" width="60%"></center>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                 We proposed a cross-context visual imitation learning method, which consists of three models: context translation model that translates the observation image to the context of robot from a different context; depth prediction model that generates predictive depth observation to provide depth input for the inverse model; multi-modal inverse dynamics model that maps the multi-modal observations into actions to reproduce the behaviours in demonstrations.</p>
              
                <div id="demo-video"></div>
                <br><br><h1>Video</h1>
                <iframe width="880" height="495" src="https://www.youtube.com/embed/5z5C8prTRmg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

                <div id="data-code"></div>
                <br><br><h1>Paper</h1>
                <a href='https://ieeexplore.ieee.org/abstract/document/9196868'>PDF</a>
                <br><br><br>
              </div>
      <!-- ENDS Footer -->
  </body>
</html>
