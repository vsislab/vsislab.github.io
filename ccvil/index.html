
<html>
<title>Cross-context Visual Imitation Learning from Demonstrations</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--Import Google Icon Font-->
      <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <!--Import materialize.css-->
      <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
      <!--Let browser know website is optimized for mobile-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>    

<body bgcolor="#F5F5F5">
	<br />
<center><h4 class="black-text" style="padding-left: 210px; padding-right: 210px;">Cross-context Visual Imitation Learning from Demonstrations</h4></center>
<center><h6 class="gray-text">Shuo Yang, Wei Zhang, Weizhi Lu, Hesheng Wang, and Yibin Li</h6></center>
<p style="padding-left: 210px;"><b>Abstract</b></p>       
<p style="padding-left: 210px; padding-right: 210px; text-align:justify; text-justify:inter-ideograph;">
Imitation learning enables robots to learn a task by simply watching the demonstration of the task. Current
imitation learning methods usually require the learner and demonstrator to occur in the same context. This limits their scalability to practical applications. In the paper, we propose a more general imitation learning method which allows the learner and the demonstrator to come from different contexts, such as different viewpoints, backgrounds, and object positions and appearances. Specifically, we design a robotic system consisting of three models: context translation model, depth prediction model and multi-modal inverse dynamics model. First, the context translation model translates the demonstration to the context of learner from a different context. Then combining the color observation and depth observation as inputs, the inverse model maps the multi-modal observations into actions to reproduce the demonstration. The depth observation is provided by a depth prediction model. By performing block stacking tasks both in simulation and real world, we prove the cross-context learning advantage of the proposed robotic system over other systems.
</p>
<!--p style="padding-left: 210px;"><b>● Data: <a href="data.html" target="_blank">View our datasets</a></b></p-->
<center><img src="image/overview.png" width="40%">
<br> Overview of our approach
</center>

<p style="padding-left: 210px;"><b>● Video:</b></p>
<!--h5 style="padding-left: 210px;"><blockquote>Explicit method for robot imitation learning</blockquote></h5>
<div style="padding-left: 210px; padding-right: 210px;"><hr></div-->
<center>
 <iframe width="560" height="315" src="https://www.youtube.com/embed/5z5C8prTRmg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  <br><br>
  
</center>

<!--iframe width="640" height="359" src="https://www.youtube.com/embed/uaJXXVa31wU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe-->
<br><br>


<!--h5 style="padding-left: 210px;"><blockquote>Implicit method for robot imitation learning</blockquote></h5
Simulation and real-world experiments video of our implicit method for robot imitation learning
Real world experiments video of our explicit method for robot imitation learning
-->
<!--Import jQuery before materialize.js-->
      <script type="text/javascript" src="js/jquery-2.1.1.min.js"></script>
      <script type="text/javascript" src="js/materialize.min.js"></script>
</body>
</html>