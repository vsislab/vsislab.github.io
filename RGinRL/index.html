
<html>
<title>Addressing  Reward  Gaming  Issue  in  Reinforcement  Learning  under Temporary  Human  Guidance </title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <!--Import Google Icon Font-->
      <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
      <!--Import materialize.css-->
      <link type="text/css" rel="stylesheet" href="css/materialize.min.css"  media="screen,projection"/>
      <!--Let browser know website is optimized for mobile-->
      <meta name="viewport" content="width=device-width, initial-scale=1.0"/>    

<body>
	<br />
<center><h4 class="black-text" style="padding-left: 300px; padding-right: 300px;">Addressing  Reward  Gaming  Issue  in  Reinforcement  Learning  under Temporary  Human  Guidance</h4></center>
<center><h6 class="gray-text">Jiapeng Sheng, Wei Zhang, Ran Song, Jiyu Cheng, and Yibin Li</h6></center>
    <center><h8>School of Control Science and Engineering, Shandong University&nbsp;&nbsp;</h8></center>
<p style="padding-left: 210px;"><b>Abstract</b></p>       
<p style="padding-left: 210px; padding-right: 210px; text-align:justify; text-justify:inter-ideograph;"><h7>
It is important but challenging for an AI agent to reliably learn desirable behaviors in a reinforcement learning (RL) system as the agent could make use of the designing loophole of the reward functions. In this paper, we study the reward gaming issue in RL that the agent misinterprets the reward function and may find novel solutions with high reward but low useful behaviors. To ``do the right thing" in the current RL environments, we propose to force the agent to explore under temporary human guidance to correct undesired behaviors when encountering reward gaming issue. Furthermore, we introduce expert relay to continuously upgrade the expert policy so that the policy can be better guided to alleviate sharp performance decline after bounding the exploration region. Experimental results show that the proposed method can effectively address reward gaming issue and help the agent reliably behave as desired.

</p>
<!--p style="padding-left: 210px;"><b>● Data: <a href="data.html" target="_blank">View our datasets</a></b></p-->
<center><img src="image/introduction.png" width="70%">
<br> Overview of the proposed method.
</center>
<p style="padding-left: 210px;"><b>● Code: </b>Coming soon!</p>
<p style="padding-left: 210px;"><b>● Video:</b></p>
<!--h5 style="padding-left: 210px;"><blockquote>Explicit method for robot imitation learning</blockquote></h5>
<div style="padding-left: 210px; padding-right: 210px;"><hr></div-->

<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/o9sOzGViwGc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    <br><br>
  



<!--h5 style="padding-left: 210px;"><blockquote>Implicit method for robot imitation learning</blockquote></h5
Simulation and real-world experiments video of our implicit method for robot imitation learning
Real world experiments video of our explicit method for robot imitation learning
-->
<!--Import jQuery before materialize.js-->
      <script type="text/javascript" src="js/jquery-2.1.1.min.js"></script>
      <script type="text/javascript" src="js/materialize.min.js"></script>
</body>
</html>