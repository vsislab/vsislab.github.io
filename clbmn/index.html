
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Learning to Navigate Sequential Environments: A Continual Learning Benchmark for Multi-modal Navigation</title>
    <!-- CSS -->
        <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen" />
    <!-- ENDS CSS -->

        <!--script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');

        </script-->

        <!-- ENDS JS -->  
  </head> 
  <body>
    <!-- MAIN -->
    <div id="main">

            <div id="match-nav-wrapper">
                <div id="match-nav-bar">

                    <table>
                        <thead>
                            <tr>
                                <!--th width="10%"><a href="#">Home</a></th-->
                                <!--th width="30%"><a href="#video-explicit">Demonstration of the explicit method</a></th>
                                <th width="30%"><a href="#video-implicit">Demonstration of the implicit method</a></th-->
                                <!--th width="15%"></th-->
                            </tr>
                        </thead>
                    </table>

                </div>
            </div>

      <!-- HEADER -->
      <div id="home-header">
      
            <div id="match-wrapper">

                <!--h5><a href="#"><img src="image/logo.png"></a></h5-->

              

                <h2>Learning to Navigate Sequential Environments: A Continual Learning Benchmark for Multi-modal Navigation</h2>
                <center>
                  Yinduo Xie, Yuenan Zhao, Qian Zhang,  Lin Zhang, Teng Li, Wei Zhang<sup>*</sup><br />
                  <font size=2>School of Control Science and Engineering, Shandong University</font>    
                </center>
                <br><strong>Abstract</strong>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                Existing works for robot navigation typically focus on performance in specific tasks, overlooking the adaptability of navigation strategies to sequential environments. In this paper, we present CLBMN, a continual learning (CL) benchmark for multi-modal robot navigation, which aims to enhance robots' navigation capabilities in handling sequential tasks. The benchmark is composed of a multi-modal dataset, a CL-based algorithm, and three comprehensive evaluation metrics for CL-based multi-modal navigation. Specifically, we construct a multi-modal dataset with 10,000 samples by collecting images, point clouds, and odometry data from five scenarios, including daytime, nighttime, rainy conditions, highway, and field. Then, we propose an end-to-end navigation framework with momentum update to mitigate catastrophic forgetting across various environments, which provides a baseline approach for CL-based robot navigation. Furthermore, we define three evaluation metrics specifically designed to evaluate the robot's continual navigation performance across different scenarios. Experimental results demonstrate that the proposed benchmark is effective for studying multi-modal continual robot navigation in sequential environments.
				<!-- <a href="https://drive.google.com/file/d/19jLcdDeCWlogXNWBIeiOquHgxtOa8JzU/view?usp=drive_link">Datasets Available Here1</a></p> -->
		     		 <!-- <a href="https://pan.baidu.com/s/1pKwUuy4jNzCJ04dYc_nIzg?pwd=m2m6">Datasets Available Here2</a></p> -->

                <h1>Overview</h1>
                <div id="ships-structure-img"><img src="images/pic2-5.png" style="width: 100%; height: auto;"></div>
		   
		<h1>Dataset</h1> 

		<p>The dataset is collected from five scenarios: daytime, nighttime, rainy conditions, highway, and field, comprising a total of 10,000 samples. As shown in the figure below, each sample includes images, point clouds, and odometry data.</p>
		<div id="ships-structure-img" style="text-align: center;"><img src="images/pic1.jpg" style="width: 80%; height: auto;"></div>
	        <p>This dataset is made freely available to academic and non-academic entities for non-commercial purposes such as academic research, teaching, scientific publications, or personal experimentation. Permission is granted to use the data given that you agree and completion of a file sent to us called "<strong>AgrenmentForMMN.docx</strong>":</p><br>
		<ol>
		    <li><p>1. Although every effort has been made to ensure accuracy through manual inspection, we (Shandong University, VSISLab) do not accept any responsibility for user-defined biases in the released images. That you include a reference to the MMN Dataset in any work that makes use of the dataset.</p></li>
		    <li><p>2. You agree not to further copy, publish or distribute any portion of the MMN dataset. Except, for internal use at a single site within the same organization it is allowed to make copies of the dataset.</p></li>
		    <li><p>3. The Visual, Sensing and Intelligent System Lab of Shandong University reserves the right to terminate your access to the database at any time.</p></li>
		    <li><p>4. The dataset only collects and summarizes publicly available forum images on the internet for data type annotation. If you feel that any image in this dataset is offensive to yourself or violates your privacy, please contact us (email: info@vsislab.com).</p></li>
		</ol>
		    
			
		    
             
		    

              </div>
      <!-- ENDS Footer -->
  </body>
</html>
