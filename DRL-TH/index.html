
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>DRL-TH: Learning to Navigate via Temporal Graph Attention and Hierarchical Fusion in Crowded Environments</title>
    <!-- CSS -->
        <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/style.css" type="text/css" media="screen" />
    <!-- ENDS CSS -->

        <!--script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');

        </script-->

        <!-- ENDS JS -->  
  </head> 
  <body>
    <!-- MAIN -->
    <div id="main">

            <div id="match-nav-wrapper">
                <div id="match-nav-bar">

                    <table>
                        <thead>
                            <tr>
                                <!--th width="10%"><a href="#">Home</a></th-->
                                <!--th width="30%"><a href="#video-explicit">Demonstration of the explicit method</a></th>
                                <th width="30%"><a href="#video-implicit">Demonstration of the implicit method</a></th-->
                                <!--th width="15%"></th-->
                            </tr>
                        </thead>
                    </table>

                </div>
            </div>

      <!-- HEADER -->
      <div id="home-header">
      
            <div id="match-wrapper">

                <!--h5><a href="#"><img src="image/logo.png"></a></h5-->

              

                <h2>DRL-TH: Learning to Navigate via Temporal Graph Attention and Hierarchical Fusion in Crowded Environments</h2>
                <center>
		       Anonymous submission
<!--                   Ruitong Li<sup>1</sup>, Yuenan Zhao<sup>1</sup>, Xiaoyu Xu<sup>1</sup>, Jiaming Chen<sup>2</sup>, Ran Song<sup>1</sup>, Wei Zhang<sup>1</sup><br />
                  <font size=2>1 School of Control Science and Engineering, Shandong University</font>  
                  <font size=2>2 Department of Computer Science, The University of Manchester</font>  -->
                </center>
                <br><strong>Abstract</strong>
                <p style="text-align:justify; text-justify:inter-ideograph;">
                Deep reinforcement learning (DRL) methods have demonstrated potential for autonomous navigation and obstacle avoidance of unmanned ground vehicles (UGVs) in crowded environments. Most existing approaches rely on single-frame observations and employ static feature weighting or simple concatenation for multi-modal fusion, which limits their ability to capture temporal context and hinders dynamic adaptability.
To address these challenges, we propose a DRL-based navigation framework, DRL-TH, which leverages temporal graph attention and hierarchical graph pooling to integrate historical observations and adaptively fuse multi-modal information. Specifically, we introduce a temporal-guided graph attention network (TG-GAT) that incorporates temporal weights into attention scores to capture correlations between consecutive frames, thereby enabling the tracking of obstacle motion trends. In addition, we design a graph hierarchical abstraction module (GHAM) that applies hierarchical pooling and learnable weighted fusion to dynamically integrate RGB and LiDAR features, achieving balanced representation across multiple scales. 
Extensive experiments demonstrate that our DRL-TH outperforms existing methods in various crowded environments. We also implemented DRL-TH control policy on a real UGV and showed that it performed well in real-world scenarios. 
                <a href="https://github.com/Bennyliiiiii/PAPNet">Code Available Here.</a></p>
		
		<br><br>
		
                <h1>Pipeline of DRL-TH</h1>
                <div id="drl-th"><img src="image/pipline.png" style="width: 100%; height: auto;"></div>
                <br><br>
		    
               <!--   <h1>Architecture of PCM</h1>
                <div id="pcm"><img src="image/papnet2.png" style="width: 100%; height: auto;"></div>  -->
		   
		    
                <br><br>      
		    
                <h1>Supplemental Video</h1>
		    <div style="text-align:center;">
		        <iframe width="800" height="365" src="https://www.youtube.com/embed/0--yPZmDaLw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                <br><br>
                <br><br>
                    </div>
	            
      <!-- ENDS Footer -->
  </body>
</html>


