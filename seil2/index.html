
<!DOCTYPE html>
<html>

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Learning Actions from Human Demonstration Video for Robotic Manipulation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="css/w3.css">
</head>

<body>

<br>

<div class="w3-container" id="paper">
  <div class="w3-content" style="max-width:850px">
    <h1 class="w3-center w3-padding-32"><b>Learning Actions from Human Demonstration Video for Robotic Manipulation</b></h1>
	<center>
        Shuo Yang, Wei Zhang, Weizhi Lu, Hesheng Wang, and Yibin Li<br>
		<font >School of Control Science and Engineering, Shandong University, 250061, Jinan, China</font>                
    </center>
	<br>
	<center>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019</center>
    <h3><b>Abstract</b></h3>
    <p style="text-align:justify; text-justify:inter-ideograph;">
    Learning actions from human demonstration is an emerging trend for designing intelligent robotic systems, which can be referred as video to command. The performance of such approach highly relies on the quality of video captioning. However, the general video captioning methods focus more on
the understanding of the full frame, lacking of consideration on the specific object of interests in robotic manipulations.
We propose a novel deep model to learn actions from human demonstration video for robotic manipulation. It consists of
two deep networks, grasp detection network (GNet) and video captioning network (CNet). GNet performs two functions:
providing grasp solutions and extracting the local features for the object of interests in robotic manipulation. CNet outputs
the captioning results by fusing the features of both full frames and local objects. Experimental results on UR5 robotic arm
show that our method could produce more accurate command from video demonstration than state-of-the-art work, thereby
leading to more robust grasping performance.
	</p>
	<center><img src="image/overview.png" style="width: 60%;"><br>Overview of the proposed method.</center>
	<!-- <h3><b>Overview of our heuristic deep reinforcement learning model for variable-sized 3D product packing</b></h3> -->
    <h3 class="w3-left-align"><b>Video</b></h3>
    <iframe width="850" height="478" src="https://www.youtube.com/embed/4ltAhXA82ds" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br> 
    <br> 
	<h3><b>Paper</b></h3>
	<a href='https://ieeexplore.ieee.org/abstract/document/8968278'>PDF</a>
    <br> 

  </div>
</div>

<br><br><br>


</body>
</html>
